---
title: "Introduction to my homework"
author: "SA24168406"
date: "2024-12-06"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to my homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Example 1
## Question
已知一组人体身高和体重的样本数据，分析身高与体重之间的关系，创建一个人体身高与体重的预测模型。

## Answer
基本思路：可以利用线性回归来对人体的身高和体重之间的关系进行建模。利用R语言线性回归函数lm()来创建关系模型，利用summary()函数获取关系模型的概要，最后使用predict()函数来预测人的体重。

```{r}
# 样本数据
x <- c(151, 174, 138, 186, 128, 136, 179, 163, 152, 131) #身高，单位cm
y <- c(63, 81, 56, 91, 47, 57, 76, 72, 62, 48)           #体重，单位kg

#调用lm()函数
ans <- lm(y~x)

#调用summary()函数
summary(ans)$coef
```
The $R^2$ is `r summary(ans)$r.squared` \
从P值来看，自变量x对因变量y存在显著性影响关系；从模型拟合指标$R^2$来看，很接近于1，说明模型中的自变量对因变量变异的解释很好。

```{r}
plot(ans)

# 调用predict()判断身高为170cm的体重
a <- data.frame(x = 170)
result <-  predict(ans,a)
print(result)
```
从诊断图Residuals vs Leverage上来看，观测值4和7落在虚线之外，说明数据集中存在过度影响的点。\
从QQ图上来看，残差呈正态分布。\
从Scale-Location图来看，红线在绘图上并不完全水平，但它在任何点都不会偏离太远。所以不违反等方差的假设。\
从Residuals vs Fitted图来看，红线偏离了完美的水平线，但并不明显。可以说残差遵循大致线性模式，并且线性回归模型适合该数据集。

## 生成回归线
```{r}
plot(x,y,col = "blue",main = "Height & Weight Regression",abline(ans),cex = 1.3,pch = 16,ylab = "Weight in Kg",xlab = "Height in cm")
```

## 线性回归方程公式
$$Y=\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_nX_n+\varepsilon$$
其中，$Y$是因变量，$X_1$,$X_2$,...,$X_n$是自变量，$\beta_0$,$\beta_1$,$\beta_2$,...,$\beta_n$是回归系数，$\varepsilon$是误差项。回归系数表示自变量对因变量的影响程度。

## Example 2
## Question
考虑如下的方差分析问题。某工厂要比较三种不同组装工艺(A,B,C)的工作效率，将15名工人随机分为3个组，每个组采用一种工艺，一周后各组的成品数如下：
```{r}
tab <- matrix(c(58,58,48,64,69,57,55,71,59,66,64,47,67,68,49),ncol=3,byrow=TRUE)
colnames(tab) <- c('A','B','C')
knitr::kable(tab)
```

## Answer
基本分析：该问题属于单因素方差分析例子，单因素方差分析可以看成基础统计中两样本t检验的一个推广，要比较试验观测值的某个因变量（称为“指标”）按照一个分组变量（称为“因素”）分组后，各组的因变量均值有无显著差异。\
设因素A将所有观测分为m个组，每组对因变量进行r次观测，且各次观测相互独立，模型为
$$y_{ij}=\mu_i+e_{ij},i=1,2,...,m,j=1,2,...,r.$$
$$e_{ij} \sim N(0,\sigma^2)$$
希望研究如下问题：
$$H_0:\mu_1=\mu_2=...=\mu_m.$$
如果拒绝$H_0$，希望找出哪些组的均值两两之间是有显著差异的。
先将上表转换成长表格式：
```{r}
library(tidyr)
library(dplyr)
x <- as.data.frame(tab)
long_table <- pivot_longer(x, cols = A:C, names_to = "grp", values_to = "value")
long_table <- arrange(long_table,grp)
print(long_table)
```
用aov()函数进行方差分析：
```{r}
res <- aov(value ~ grp, data=long_table)
summary(res)
```
主效应grp的F检验的p值为0.004， 若检验水平为0.05则分组效应显著， 各组之间有显著差异。\
可以用并列盒形图比较各组的取值，并检查分布对称性和方差齐性：
```{r}
library(ggplot2)
ggplot(data = long_table, mapping = aes(x = grp, y = value)) + geom_boxplot()
```
\
从图形看，C组的工作效率显著低于A组和B组，A、B两组之间差异不明显。\
为了检查各组方差是否相等，可以进行Bartlett检验：
```{r}
bartlett.test(value ~ grp, data = long_table)
```
结果表明可以认为各组的方差相等。

## Example 3
## Question
两因素方差分析计算示例：以R的datasets包的ToothGrowth数据为例。考虑维生素C对豚鼠的成牙质细胞生长的影响，因变量(指标)len是某牙齿长度测量值，考虑两种不同的维生素C类型(变量supp，取值为OJ或VC)，以及三种剂量(变量dose，取值为0.5, 1.0, 2.0)。

## Answer
基本思路：两因素方差分析，最简单的情形是有两个分组变量（称为因素）A和B，A有s个分类，B有t个分类，对A和B的每一种搭配(i,j)，重复试验r次得到观测值$y_{ijk}$，设各次观测值相互独立。模型为
$$y_{ijk}=\mu_{ij}+e_{ijk},\;i=1,...,s,\;j=1,...,t,\;k=1,...,r,$$
误差项$e_{ijk}$独立同正态分布$N(0,\sigma^2)$。
一般将上述模型用另外的参数表示成：
$$y_{ijk}=\mu+\alpha_i+\beta_j+\gamma_{ij}+e_{ijk},$$
其中$\alpha_i$表示因素A的“主效应”，$\beta_j$表示因素B的主效应，$\gamma_{ij}$表示因素A和因素B的交互作用效应。这样模型中的参数是冗余的，一般会加限制，比如设$\alpha_1=0$,$\beta_1=0$,$\gamma_{i1}=\gamma_{1j}=0$。
二元方差分析的问题是分别检验两个因素的主效应是否显著（不等于零），交互作用效应是否存在（不等于零）。
```{r}
library(dplyr)
data(ToothGrowth, package="datasets")
d.tooth <- ToothGrowth
mutate(d.tooth, supp = factor(supp, levels=c("OJ", "VC")),dose = factor(dose, levels=c(0.5, 1.0, 2.0)))
x <- count(d.tooth, supp, dose)
knitr::kable(x)
```
可见这个试验是均衡有重复完全试验设计。
进行二元方差分析：
```{r}
aov.to1 <- aov(len ~ dose + supp + dose:supp, data = d.tooth)
summary(aov.to1)
```
在0.05水平下，两个主效应和交互作用效应都显著。

## Exercise 3.4
## Question
The Rayleigh density is
$$f(x)=\frac{x}{\sigma^2}e^{-x^2/(2\sigma^2)},\qquad x\ge0,\,\sigma>0.$$
Develop an algorithm to generate random samples from a Rayleigh(σ) distribution.Generate Rayleigh(σ) samples for several choices of σ > 0 and check that the mode of the generated samples is close to the theoretical mode σ(check the histogram).

## Answer
基本思路：可以利用逆变换法生成Rayleigh分布的随机数。该方法基本原理是：对于连续性随机变量$X$，其分布函数$F(x)$也是一个随机变量，服从参数为0，1的均匀分布$U(0,1)$，则依据概率论知识，若$u\sim U(0,1)$，则$F^{-1}(u)\sim F(x)$。
Rayleigh分布的分布函数：
$$F(x)=1-e^{-x^2/(2\sigma^2)}$$
令其为$u$，$u\sim U(0,1)$
最后可得
$$x=\sqrt{-2\sigma^2ln(1-u)}$$
随机变量$x$即为Rayleigh分布的随机数。
实现代码如下：
```{r}
set.seed(1)
u <- runif(1000)
s <- 0.5  # s表示参数sigma，这里取值0.5
x <- sqrt(-2*(s^2)*log(1-u))
```
然后利用直方图检查生成样本的模式是否接近理论模式$\sigma$
```{r}
hist(x,breaks = 50,freq = F,main = expression(f(x)==4*x*e^(-2*x^2)))
lines(density(x))
```
\
结论：生成的样本服从$\sigma=0.5$的Rayleigh分布。\

参数$\sigma$取不同值的情况：
```{r}
set.seed(2)
u <- runif(1000)
s <- 1  # s表示参数sigma，这里取值1
x <- sqrt(-2*(s^2)*log(1-u))
hist(x,breaks = 50,freq = F,main = expression(f(x)==x*e^(-x^2/2)))
lines(density(x))
```
\
结论：生成的样本服从$\sigma=1$的Rayleigh分布。\

```{r}
set.seed(3)
u <- runif(1000)
s <- 2  # s表示参数sigma，这里取值2
x <- sqrt(-2*(s^2)*log(1-u))
hist(x,breaks = 50,freq = F,main = expression(f(x)==frac(x,4)*e^(-x^2/8)))
lines(density(x))
```
\
结论：生成的样本服从$\sigma=2$的Rayleigh分布。\

## Exercise 3.11
## Question
Generate a random sample of size 1000 from a normal location mixture.The components of the mixture have N(0,1) and N(3,1) distributions with mixing probabilities p1 and p2 = 1 − p1.Graph the histogram of the sample with density superimposed,for p1 = 0.75.Repeat with different values for p1 and observe whether the empirical distribution of the mixture appears to be bimodal.Make a conjecture about the values of p1 that produce bimodal mixtures.

## Answer
基本思路：先分别从N(0,1)和N(3,1)中各随机抽取1000个样本，然后分别按照可能性p1和p2进行混合，生成新的1000个随机样本，这些样本服从混合正态分布。它的概率密度函数为
$$f(x)=\frac{p1}{\sqrt{2\pi}}e^{-x^2/2}+\frac{p2}{\sqrt{2\pi}}e^{-(x-3)^2/2}$$
可能性p1和p2可以用sample函数来描述，利用sample函数从(1,0)中随机抽样1000次，抽取1的概率权重为p1，抽取0的概率权重为p2。
实现代码如下：
```{r}
set.seed(1)
N1 <- rnorm(1000,0,1)
N2 <- rnorm(1000,3,1)
p1 <- 0.75
p2 <- 1-p1
p <- sample(c(1,0),1000,replace=TRUE,prob = c(p1,p2))
X <- p*N1+(1-p)*N2
```
绘制样本的频率分布直方图：
```{r}
hist(X,breaks = 50,freq = F,main = expression(f(x)==frac(0.75,sqrt(2*pi))*e^(-x^2/2)+frac(0.25,sqrt(2*pi))*e^(-(x-3)^2/2)))
lines(density(X))
```
\
然后改变p1的值，重复上面的步骤，观察不同的直方图情况：  
p1=0.1
```{r}
set.seed(1)
N1 <- rnorm(1000,0,1)
N2 <- rnorm(1000,3,1)
p1 <- 0.1
p2 <- 1-p1
p <- sample(c(1,0),1000,replace=TRUE,prob = c(p1,p2))
X <- p*N1+(1-p)*N2
hist(X,breaks = 50,freq = F,main = expression(f(x)==frac(0.1,sqrt(2*pi))*e^(-x^2/2)+frac(0.9,sqrt(2*pi))*e^(-(x-3)^2/2)))
lines(density(X))
```
  
p1=0.2
```{r}
set.seed(1)
N1 <- rnorm(1000,0,1)
N2 <- rnorm(1000,3,1)
p1 <- 0.2
p2 <- 1-p1
p <- sample(c(1,0),1000,replace=TRUE,prob = c(p1,p2))
X <- p*N1+(1-p)*N2
hist(X,breaks = 50,freq = F,main = expression(f(x)==frac(0.2,sqrt(2*pi))*e^(-x^2/2)+frac(0.8,sqrt(2*pi))*e^(-(x-3)^2/2)))
lines(density(X))
```
  
p1=0.3
```{r}
set.seed(1)
N1 <- rnorm(1000,0,1)
N2 <- rnorm(1000,3,1)
p1 <- 0.3
p2 <- 1-p1
p <- sample(c(1,0),1000,replace=TRUE,prob = c(p1,p2))
X <- p*N1+(1-p)*N2
hist(X,breaks = 50,freq = F,main = expression(f(x)==frac(0.3,sqrt(2*pi))*e^(-x^2/2)+frac(0.7,sqrt(2*pi))*e^(-(x-3)^2/2)))
lines(density(X))
```
  
p1=0.4
```{r}
set.seed(1)
N1 <- rnorm(1000,0,1)
N2 <- rnorm(1000,3,1)
p1 <- 0.4
p2 <- 1-p1
p <- sample(c(1,0),1000,replace=TRUE,prob = c(p1,p2))
X <- p*N1+(1-p)*N2
hist(X,breaks = 50,freq = F,main = expression(f(x)==frac(0.4,sqrt(2*pi))*e^(-x^2/2)+frac(0.6,sqrt(2*pi))*e^(-(x-3)^2/2)))
lines(density(X))
```
  
p1=0.5
```{r}
set.seed(1)
N1 <- rnorm(1000,0,1)
N2 <- rnorm(1000,3,1)
p1 <- 0.5
p2 <- 1-p1
p <- sample(c(1,0),1000,replace=TRUE,prob = c(p1,p2))
X <- p*N1+(1-p)*N2
hist(X,breaks = 50,freq = F,main = expression(f(x)==frac(0.5,sqrt(2*pi))*e^(-x^2/2)+frac(0.5,sqrt(2*pi))*e^(-(x-3)^2/2)))
lines(density(X))
```
  
p1=0.6
```{r}
set.seed(1)
N1 <- rnorm(1000,0,1)
N2 <- rnorm(1000,3,1)
p1 <- 0.6
p2 <- 1-p1
p <- sample(c(1,0),1000,replace=TRUE,prob = c(p1,p2))
X <- p*N1+(1-p)*N2
hist(X,breaks = 50,freq = F,main = expression(f(x)==frac(0.6,sqrt(2*pi))*e^(-x^2/2)+frac(0.4,sqrt(2*pi))*e^(-(x-3)^2/2)))
lines(density(X))
```
  
p1=0.7
```{r}
set.seed(1)
N1 <- rnorm(1000,0,1)
N2 <- rnorm(1000,3,1)
p1 <- 0.7
p2 <- 1-p1
p <- sample(c(1,0),1000,replace=TRUE,prob = c(p1,p2))
X <- p*N1+(1-p)*N2
hist(X,breaks = 50,freq = F,main = expression(f(x)==frac(0.7,sqrt(2*pi))*e^(-x^2/2)+frac(0.3,sqrt(2*pi))*e^(-(x-3)^2/2)))
lines(density(X))
```
  
p1=0.8
```{r}
set.seed(1)
N1 <- rnorm(1000,0,1)
N2 <- rnorm(1000,3,1)
p1 <- 0.8
p2 <- 1-p1
p <- sample(c(1,0),1000,replace=TRUE,prob = c(p1,p2))
X <- p*N1+(1-p)*N2
hist(X,breaks = 50,freq = F,main = expression(f(x)==frac(0.8,sqrt(2*pi))*e^(-x^2/2)+frac(0.2,sqrt(2*pi))*e^(-(x-3)^2/2)))
lines(density(X))
```
  
p1=0.9
```{r}
set.seed(1)
N1 <- rnorm(1000,0,1)
N2 <- rnorm(1000,3,1)
p1 <- 0.9
p2 <- 1-p1
p <- sample(c(1,0),1000,replace=TRUE,prob = c(p1,p2))
X <- p*N1+(1-p)*N2
hist(X,breaks = 50,freq = F,main = expression(f(x)==frac(0.9,sqrt(2*pi))*e^(-x^2/2)+frac(0.1,sqrt(2*pi))*e^(-(x-3)^2/2)))
lines(density(X))
```
  
通过观察上面9幅直方图，可以直观地发现p1的取值在[0.3,0.7]之间时，双峰分布明显。当p1<0.3或p1>0.7时，双峰不明显。

## Exercise 3.20
## Question
A compound Poisson process is a stochastic process {$X(t),t\ge0$} that can be represented as the random sum $X(t)=\begin{matrix} \sum_{i=1}^{N(t)} Y_i \end{matrix},t\ge0$, where {$N(t),t\ge0$} is a Poisson process and $Y_1$, $Y_2$,... are iid and independent of {$N(t),t\ge0$}. Write a program to simulate a compound Poisson(λ)–Gamma process (Y has a Gamma distribution). Estimate the mean and the variance of $X(10)$ for several choices of the parameters and compare with the theoretical values.
Hint: Show that $E[X(t)]=\lambda tE[Y_1]$ and $Var(X(t))=\lambda tE[Y_1^2]$.

## Answer
基本思路：{$N(t),t\ge0$}是一个泊松过程，在实际中常被使用的计数随机过程，它表示到时刻t为止，已发生的事件$Y_i$的总数，该总数服从泊松分布。而$Y_i$是一独立同分布的随机变量，且与{$N(t),t\ge0$}独立，本题中是Gamma分布。而{$X(t),t\ge0$}就是这些独立同分布的随机变量的和。由此，模拟步骤如下：  
先生成1000个服从参数为$\lambda t$的泊松分布的随机变量，这些变量即为$N(t)$;  
然后根据$N(t)$的值，确定$Y_i$的生成数量，$Y_i$是服从参数已定的Gamma分布的随机变量;  
最后分别将不同$N(t)$下的$Y_i$累加，得到$X(t)$。求出其均值和方差，与理论值比较。  
实现代码如下：
```{r}
set.seed(1)
t <- 10
l <- 1            # λ=1
n <- rpois(1000,t*l)
x <- vector(mode = "numeric",length = 1000)
for (i in 1:1000) {
  y <- rgamma(n[i],shape = 1,rate = 1)
  x[i] <- sum(y)
}
x_mean_estimate <- mean(x)   # 均值的估计值
x_var_estimate <- var(x)     # 方差的估计值

x_mean_theoretical <- l*t*mean(y)    # 均值的理论值
x_var_theoretical <- l*t*mean(y^2)   # 方差的理论值

sprintf("x_mean_estimate: %f",x_mean_estimate)
sprintf("x_mean_theoretical: %f",x_mean_theoretical)
sprintf("x_var_estimate: %f",x_var_estimate)
sprintf("x_var_theoretical: %f",x_var_theoretical)
```
改变参数$\lambda$的值，再求均值和方差，与理论值比较。  
$\lambda=5$
```{r}
set.seed(1)
t <- 10
l <- 5            # λ=5
n <- rpois(1000,t*l)
x <- vector(mode = "numeric",length = 1000)
for (i in 1:1000) {
  y <- rgamma(n[i],shape = 1,rate = 1)
  x[i] <- sum(y)
}
x_mean_estimate <- mean(x)   # 均值的估计值
x_var_estimate <- var(x)     # 方差的估计值

x_mean_theoretical <- l*t*mean(y)    # 均值的理论值
x_var_theoretical <- l*t*mean(y^2)   # 方差的理论值

sprintf("x_mean_estimate: %f",x_mean_estimate)
sprintf("x_mean_theoretical: %f",x_mean_theoretical)
sprintf("x_var_estimate: %f",x_var_estimate)
sprintf("x_var_theoretical: %f",x_var_theoretical)
```
$\lambda=10$
```{r}
set.seed(1)
t <- 10
l <- 10            # λ=10
n <- rpois(1000,t*l)
x <- vector(mode = "numeric",length = 1000)
for (i in 1:1000) {
  y <- rgamma(n[i],shape = 1,rate = 1)
  x[i] <- sum(y)
}
x_mean_estimate <- mean(x)   # 均值的估计值
x_var_estimate <- var(x)     # 方差的估计值

x_mean_theoretical <- l*t*mean(y)    # 均值的理论值
x_var_theoretical <- l*t*mean(y^2)   # 方差的理论值

sprintf("x_mean_estimate: %f",x_mean_estimate)
sprintf("x_mean_theoretical: %f",x_mean_theoretical)
sprintf("x_var_estimate: %f",x_var_estimate)
sprintf("x_var_theoretical: %f",x_var_theoretical)
```
结论：经过不同参数的比较，可以发现均值和方差的估计值与公式求的理论值非常接近。

## Exercise 5.4
## Question
Write a function to compute a Monte Carlo estimate of the Beta(3,3) cdf, and use the function to estimate F(x) forx=0.1,0.2,...,0.9. Compare the estimates with the values returned by the pbeta function in R.

## Answer
利用Monte Carlo方法估计Beta(3,3)的累积分布函数思路如下：
$$\theta=\int_{0}^{x}\frac{1}{B(\alpha,\beta)}y^{\alpha-1}(1-y)^{\beta-1}\mathrm{d}y=E_Y[x\frac{1}{B(\alpha,\beta)}Y^{\alpha-1}(1-Y)^{\beta-1}],\qquad Y\sim U(0,x)$$
由Monte Carlo方法得：
$$\hat{\theta}=\frac{1}{m}x\sum_{i=1}^m \frac{1}{B(\alpha,\beta)}y^{\alpha-1}(1-y)^{\beta-1}$$
实现代码如下：
```{r}
MyFun <- function(x)
{
  a <- 3
  b <- 3
  n <- 1000
  y <- runif(n,0,x)
  F_est <- x*mean(1/beta(a,b)*y^(a-1)*(1-y)^(b-1))
  return(F_est)
}
```
然后使x=0.1,0.2,...,0.9，分别求出估计值与pbeta函数算出的值进行比较：
```{r}
x <- c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)
F_estimate <- numeric(length(x))
F_pbeta <- numeric(length(x))
for (i in 1:length(x)) {
  F_estimate[i] <- MyFun(x[i])  # 利用我们写的函数计算F(x)
  F_pbeta[i] <- pbeta(x[i],3,3) # 利用r语言自带的pbeta函数计算F(x)
}
print(round(rbind(x, F_estimate, F_pbeta), 3))
```
结果分析：利用Monte Carlo方法估计的F(x)值非常接近于pbeta函数计算的值。

## Exercise 5.9
## Question
The Rayleigh density is
$$f(x)=\frac{x}{\sigma^2}e^{-x^2/(2\sigma^2)},\qquad x\ge0,\,\sigma>0.$$
Implement a function to generate samples from a Rayleigh(σ) distribution, using antithetic variables. What is the percent reduction in variance of $\frac{x+x^\prime}{2}$ compared with $\frac{x_1+x_2}{2}$ for  independent $X_1$,$X_2$?

## Answer
基本思路：根据之前课上学的生成随机数的方法——逆变换法，同时结合对偶变数法来生成Rayleigh分布的随机数，与不使用对偶变数法生成Rayleigh分布的随机数进行对比，比较两者的方差。  
实现代码如下：
```{r}
set.seed(1)
MyFunRay <- function(m=1000,antithetic=TRUE,sigma=1){  ##默认10000个抽样，使用对偶变数法，参数sigma=1
  a <- runif(m/2)
  if(antithetic)
    b <- 1-a
  else
    b <- runif(m/2)
  n <- c(a,b)
  Ray <- mean(sqrt(-2*sigma^2*log(1-n)))  ##逆变换法生成随机数，并求均值(Monte Carlo)
  return(Ray)
}

t <- 1000
Ray1 <- Ray2 <- numeric(t)
for (i in 1:t) {
  Ray1[i] <- MyFunRay(antithetic = FALSE)  ##不使用对偶变数法
  Ray2[i] <- MyFunRay()  ##使用对偶变数法
}
print(round(c(var(Ray1),var(Ray2),(var(Ray1) - var(Ray2))/var(Ray1)),9))
```
结果分析：利用对偶变数法可以减少方差，达到了约95%的减少度。

## Exercise 5.13
## Question
Find two importance functions $f_1$ and $f_2$ that are supported on (1,$\infty$)and are ‘close’ to
$$g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2},\qquad x>1.$$
Which of your two importance functions should produce the smaller variance in estimating
$$\int_{1}^{\infty}\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}\mathrm{d}x$$
by importance sampling? Explain.

## Answer
基本思路：根据重要性采样方法的原理，我找到的两个接近于g(x)的函数$f_1$和$f_2$如下：
$$f_1(x)=\frac{x^2}{\sqrt{2\pi}},\qquad x>1$$
$$f_2(x)=xe^{-x^2/2},\qquad x>1$$ 
实现代码如下：
```{r}
set.seed(1)
m <- 10000
theta.hat <- se <- numeric(3)
va <- numeric(2)
g <- function(x){
  x^2/sqrt(2*pi)*exp(-x^2/2) * (x>1)
}

x <- runif(m,1,5)
fg <- g(x)
theta.hat[1] <- mean(fg)
se[1] <- sd(fg)

x <- runif(m,1,5)
f1 <- g(x)/exp(-x^2/2)  ##f1函数
theta.hat[2] <- mean(f1)
se[2] <- sd(f1)

x <- runif(m,1,5)
f2 <- g(x)/(x/sqrt(2*pi))  ##f2函数
theta.hat[3] <- mean(f2)
se[3] <- sd(f2)

va[1] <- var(abs(fg-f1))  ##f1产生的方差
va[2] <- var(abs(fg-f2))  ##f2产生的方差

rbind(theta.hat,se)
print(va)
```
结果分析：根据上面代码的数据可以发现函数$f_2$更接近于g(x)，在估计$\int_{1}^{\infty}\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}\mathrm{d}x$时，产生的方差更小。

##  Monte Carlo experiment
  + For $n=10^4,2\times10^4,4\times10^4,6\times10^4,8\times10^4$, apply the fast sorting algorithm to randomly permuted numbers of $1,\ldots,n$. 
  + Calculate computation time averaged over 100 simulations, denoted by $a_n$. 
  + Regress $a_n$ on $t_n:=n\log(n)$, and graphically show the results (scatter plot and regression line).
  
## Answer
```{r}
quick_sort<-function(x){
  num<-length(x)
  if(num==0||num==1){
    t[j] <<- t[j]+1
    return(x)
  }else{
    a<-x[1]
    y<-x[-1]
    lower<-y[y<a]
    upper<-y[y>=a]
    t[j] <<- t[j]+1
    return(c(quick_sort(lower),a,quick_sort(upper)))}
}

m <- 100
n <- c(1e4,2*1e4,4*1e4,6*1e4,8*1e4)
y <- numeric(length(n))
for (i in 1:length(n)) {
  t <- numeric(m)
  for (j in 1:m) {
    x <- sample(1:n[i])
    quick_sort(x)  ## 快速排序算法
  }
  y[i] <- mean(t)  ## 100次模拟的平均时间
}
x <- n*log2(n)  ## 期望时间，O(nlog2(n))
ans <- lm(y~x)

plot(x,y,col = "blue",main = "Average Estimated Time & Expected Time Regression",abline(ans),cex = 1.3,pch = 16,ylab = "computation time averaged over 100 simulations",xlab = "expected time t = nlog(n)")
```
结果分析：根据实验得出的散点图和回归线来看，快速排序算法模拟100次的平均时间与期望时间非常接近，进一步验证了快排算法的平均时间复杂度为O(nlog2(n))。

## Exercise 6.6
## Question
Estimate the 0.025, 0.05, 0.95, and 0.975 quantiles of the skewness $\sqrt{b_1}$ under normality by a Monte Carlo experiment. Compute the standard error of the estimates from (2.14) using the normal approximation for the density (with exact variance formula). Compare the estimated quantiles with the quantiles of the large sample approximation $\sqrt{b_1}\approx N(0,6/n)$.

## Answer
Estimate the 0.025 quantiles of the skewness $\sqrt{b_1}$
```{r}
set.seed(1)
m <- 1e4
n <- 10
q.hat <- numeric(m)
for (i in 1:m) {
  x <- rnorm(m,0,6/n)
  q.hat[i] <- quantile(x,0.025)  ## Monte Carlo方法估算分位数
}
q <- qnorm(0.025,0,6/n)  ## 标准分位数
print(round(c(q,mean(q.hat),sd(q.hat),var(q.hat)),4))
```
Estimate the 0.05 quantiles of the skewness $\sqrt{b_1}$
```{r}
set.seed(1)
m <- 1e4
n <- 10
q.hat <- numeric(m)
for (i in 1:m) {
  x <- rnorm(m,0,6/n)
  q.hat[i] <- quantile(x,0.05)  ## Monte Carlo方法估算分位数
}
q <- qnorm(0.05,0,6/n)  ## 标准分位数
print(round(c(q,mean(q.hat),sd(q.hat),var(q.hat)),4))
```
Estimate the 0.95 quantiles of the skewness $\sqrt{b_1}$
```{r}
set.seed(1)
m <- 1e4
n <- 10
q.hat <- numeric(m)
for (i in 1:m) {
  x <- rnorm(m,0,6/n)
  q.hat[i] <- quantile(x,0.95)  ## Monte Carlo方法估算分位数
}
q <- qnorm(0.95,0,6/n)  ## 标准分位数
print(round(c(q,mean(q.hat),sd(q.hat),var(q.hat)),4))
```
Estimate the 0.975 quantiles of the skewness $\sqrt{b_1}$
```{r}
set.seed(1)
m <- 1e4
n <- 10
q.hat <- numeric(m)
for (i in 1:m) {
  x <- rnorm(m,0,6/n)
  q.hat[i] <- quantile(x,0.975)  ## Monte Carlo方法估算分位数
}
q <- qnorm(0.975,0,6/n)  ## 标准分位数
print(round(c(q,mean(q.hat),sd(q.hat),var(q.hat)),4))
```
结果分析：使用Monte Carlo方法估算的分位数与该分布下的标准分位数非常接近，估算的分位数标准误差在0.01量级上，方差在0.0001量级上。

## Exercise 6.B
## Question
Tests for association based on Pearson product moment correlation $\rho$, Spearman’s rank correlation coefficient $\rho_s$, or Kendall’s coefficient $\tau$, are implemented in cor.test. Show (empirically) that the nonparametric tests based on $\rho_s$ or $\tau$ are less powerful than the correlation test when the sampled distribution is bivariate normal. Find an example of an alternative (a bivariate distribution (X,Y) such that X and Y are dependent) such that at least one of the nonparametric tests have better empirical power than the correlation test against this alternative.

## Answer
```{r}
set.seed(1)
library(MASS)
options(digits = 3)
m <- 1000
nump <- numk <- nums <- numeric(m)
p1 <- p2 <- p3 <- numeric(m)
for (i in 1:m) {
  mean <- c(0, 1)
  sigma <- matrix(c(1, 0.15, 0.15, 1), ncol = 2, nrow = 2)
  data <- mvrnorm(n = 500, mean, sigma)
  x <- data[, 1]
  y <- data[, 2] 
  
  a1 <- cor.test(x, y, method =  "pearson")
  a2 <- cor.test(x, y, method =  "kendall")
  a3 <- cor.test(x, y, method =  "spearman")
  
  p1[i] <- a1$p.value
  p2[i] <- a2$p.value
  p3[i] <- a3$p.value
}

nump[p1 < 0.05] <- 1
numk[p2 < 0.05] <- 1
nums[p3 < 0.05] <- 1

ratio <- c(sum(nump), sum(numk), sum(nums)) / m
print(ratio)
barplot(ratio,col = c("red","green","blue"),names.arg = c("pearson","kendall","spearman"),main = "Power of a Test")
```
结果分析：由实验结果可知，pearson的检验功效比kendall和spearman都大，这验证了题目中的这个判断：nonparametric tests based on $\rho_s$ or $\tau$ are less powerful than the correlation test when the sampled distribution is bivariate normal.

```{r}
set.seed(1)
library(MASS)
m <- 1000
n <- 1000
nump <- numk <- nums <- numeric(m)
p1 <- p2 <- p3 <- numeric(m)
for (i in 1:m) {
  mean <- c(0, 1)
  sigma <- matrix(c(2, 0, 0, 3), ncol = 2, nrow = 2)
  data <- mvrnorm(n , mean, sigma)
  x <- data[, 1]
  y <- data[, 2] + runif(n, min = 0, max = 1)
  
  a1 <- cor.test(x, y, method =  "pearson")
  a2 <- cor.test(x, y, method =  "kendall")
  a3 <- cor.test(x, y, method =  "spearman")
  
  p1[i] <- a1$p.value
  p2[i] <- a2$p.value
  p3[i] <- a3$p.value
}

nump[p1 < 0.05] <- 1
numk[p2 < 0.05] <- 1
nums[p3 < 0.05] <- 1

ratio <- c(sum(nump), sum(numk), sum(nums)) / m
print(ratio)
barplot(ratio,col = c("red","green","blue"),names.arg = c("pearson","kendall","spearman"),main = "Power of a Test")
```
结果分析：由实验结果可知，这次的kendall和spearman的检验功效比pearson的大，说明找到的二元分布(X,Y)符合题目的条件。

## Question
* Discussion

    + If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?
        
        - What is the corresponding hypothesis test problem?
        
        - Which test can we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?
        
        - Please provide the least necessary information for hypothesis testing.

## Answer
对应的假设检验问题是这两个power是否不同，也就是$H_0:power_1=power_2$,$H_1:power_1\neq power_2$。  
可以用Z-test，因为Z检验适用于总体呈正态分布，总体方差已知（或者方差未知，但样本容量够大），样本容量较大（≥30）的情况。题目条件属于方差未知，但样本容量够大，所以z的统计量为
$$z=\frac{\bar{X}-\mu_0}{S/\sqrt{n}}$$  
实现代码如下：
```{r}
U <- (0.651-0.676)/sqrt(0.651*(1-0.651))*sqrt(10000)
U <- abs(U)
print(pnorm(U, mean = 0, sd = 1, lower.tail = TRUE))  ## 检验的p值
```
结果分析：检验的p值大于显著性水平0.05，所以接受原假设$H_0:power_1=power_2$。即两个power在0.05显著性水平上相同。

* Class work 

    + Use the bootstrap method (R = 10000) to calculate the sample mean difference for comparing the following two independent samples:
    ```{r,eval=F}
    d1 <- c(-2.961, 0.478, -0.391, -0.869, -0.460, 
            -0.937, 0.779, -1.409, 0.027, -1.569);
    d2  <- c(1.608, 1.009,  0.878,  1.600, -0.263,  
             0.680, 2.280,  2.390, 1.793, 1.468)
    ```
    + The outputs should include original statistic, sample standard error, and bootstrap standard error.

## Answer
```{r}
d1 <- c(-2.961, 0.478, -0.391, -0.869, -0.460, 
        -0.937, 0.779, -1.409, 0.027, -1.569);
d2  <- c(1.608, 1.009,  0.878,  1.600, -0.263,  
         0.680, 2.280,  2.390, 1.793, 1.468);
set.seed(12345); B <- 1e4; thetastar <- numeric(B)
theta <- mean(d1)-mean(d2);
for(b in 1:B){
  d1star <- sample(d1,replace=TRUE)
  d2star <- sample(d2,replace=TRUE)
  thetastar[b] <- mean(d1star)-mean(d2star)
}
round(c(original=theta,bias=mean(thetastar)-theta,se.boot=sd(thetastar),se.samp=sd(d1-d2)/sqrt(length(d2))),3)
```

## Question
Of N =1000 hypotheses, 950 are null and 50 are alternative. The p-value under any null hypothesis is uniformly distributed (use runif), and the p-value under any alternative hypothesis follows the beta distribution with parameter 0.1 and 1 (use rbeta). Obtain Bonferroni adjusted p-values and B-H adjusted p-values. Calculate FWER, FDR, and TPR under nominal level α =0.1 for each of the two adjustment methods based on m=10000 simulation replicates. You should output the 6 numbers (3) to a 3×2 table (column names: Bonferroni correction, B-H correction; row names: FWER, FDR, TPR). Comment the results.

## Answer
```{r}
set.seed(12345)
N <- 1e3
a <- 0.1
m <- 1e4
results <- matrix(0, nrow = 3, ncol = 2)
for (i in 1:m) {
  p <- runif(950)
  p <- c(p,rbeta(50,0.1,1))
  p.adj1 <- p.adjust(p,method = "bonferroni")  # Bonferroni adjusted p-values
  p.adj2 <- p.adjust(p,method = "BH")          # B-H adjusted p-values
  reject_null1 <- p.adj1 < a
  reject_null2 <- p.adj2 < a
  FWER1 <- mean(reject_null1[1:950])           # Bonferroni校正的FWER
  FWER2 <- mean(reject_null2[1:950])           # B-H校正的FWER
  FDR1 <- mean(reject_null1)                   # Bonferroni校正的FDR
  FDR2 <- mean(reject_null2)                   # B-H校正的FDR
  TPR1 <- mean(reject_null1[951:1000])         # Bonferroni校正的TPR
  TPR2 <- mean(reject_null2[951:1000])         # B-H校正的TPR
  results[,1] <- results[,1]+c(FWER1, FDR1, TPR1)
  results[,2] <- results[,2]+c(FWER2, FDR2, TPR2)
}
results <- results / m   # 取模拟的均值
colnames(results) <- c("Bonferroni correction", "B-H correction")
rownames(results) <- c("FWER", "FDR", "TPR")
print(results)
```
分析：Bonferroni校正的FWER较低，但TPR也较低，而B-H校正的FDR和TPR较高。因为Bonferroni检验过于严格，抹杀了一切假阳性的概率，但真阳性的概率也会受到影响而降低。而B-H是相对比较温和的方法校正P值，其试图在假阳性和假阴性间达到平衡，将假/真阳性比例控制到一定范围之内，所以会出现上述情况。

## Exercise 7.4
## Question
Refer to the air-conditioning data set aircondit provided in the boot package. The 12 observations are the times in hours between failures of airconditioning equipment [63, Example 1.1]:
$$3, 5,7,18,43,85,91,98,100,130,230,487.$$
Assume that the times between failures follow an exponential model Exp(λ). Obtain the MLE of the hazard rate λ and use bootstrap to estimate the bias and standard error of the estimate.

## Answer
思路：首先利用样本算出$\lambda$，因为样本服从参数为$\lambda$的指数分布，该期望为$\frac{1}{\lambda}$，所以算出样本均值即可求出参数$\lambda$。然后利用bootstrap算法求出$\lambda^*$，再算bias和standard error of the estimate。具体实现代码如下：
```{r}
library(boot)
t(aircondit)
set.seed(12345)
B <- 1e4
x <- aircondit$hours
R <- numeric(B)
theta <- 1/mean(x)   # E(x)=1/λ
thetastar <- numeric(B)
for (b in 1:B) {
  hstar <- sample(x, replace = TRUE)
  thetastar[b] <- 1/mean(hstar)
}
round(c(MLE=theta),8)
round(c(bias=mean(thetastar)-theta,se.boot=sd(thetastar)),3)
```

## Exercise 7.5
## Question
Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the mean time between failures 1/λ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

## Answer
思路：大致过程与上题相似，只是改成使用r语言自带的boot函数和boot.ci函数来实现bootstrap算法和求置信区间。具体代码如下：
```{r}
library(boot)
t(aircondit)
set.seed(12345)
m <- 1e3;r <- 1e3;
x <- aircondit$hours
boot.mean <- function(x,i) mean(x[i])
de <- boot(data=x,statistic=boot.mean, R = r)
boot.ci(de,type=c("norm","basic","perc","bca"))
```
解释为什么这4种方法得出的区间不一样：  
使用norm方法：我们认为$\hat{\theta}$服从正态分布并且样本规模足够大，但是题目中样本规模很小。  
使用basic方法：基本的bootstrap置信区间通过减去观察到的统计量来变换复制样本的分布。  
使用perc方法：经验分布的分位数是$\hat{\theta}$抽样分布的分位数的估计量，当$\hat{\theta}$的分布不呈正态分布时，这些随机的分位数就能更接近真实的分布。  
使用bca方法：更好的置信区间是百分位区间的一种改进版本，具有更好的理论性质和更佳的实际表现。

## Question
Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$.

## Answer
思路：先按照题目要求计算出所求参数$\theta$，然后利用jackknife方法求参数估计值$\hat{\theta}$，最后根据上课所学公式求出jackknife estimates of bias and standard error of $\hat{\theta}$。
```{r}
library(bootstrap)
sigma <- cov(scor)  ## 5X5的协方差矩阵
s <- eigen(sigma)  ## 求特征值
lambda <- sort(s$values, decreasing = TRUE)  ## 按特征值降序排列
theta <- lambda[1]/sum(lambda)  ## 求参数theta
## 利用jackknife方法求参数的估计值theta.jack
n <- 88
theta.jack<-numeric(n)
for(i in 1:n){
  sigma.jack <- cov(scor[-i,])
  s.jack <- eigen(sigma.jack)
  lambda.jack <- sort(s.jack$values, decreasing = TRUE)
  theta.jack[i] <- lambda.jack[1]/sum(lambda.jack)
}
bias.jack <- (n-1)*(mean(theta.jack)-theta)  ## jackknife estimates of bias
se.jack <- sqrt((n-1)*mean((theta.jack-theta)^2))  ## jackknife estimates of se
round(c(original=theta,bias.jack=bias.jack,se.jack=se.jack),3)
```
根据代码结果显示，利用jackknife方法得出的参数估计值非常接近参数原始值，bias非常小。

## Question
In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Repeat the analysis replacing the Log-Log model with a cubic polynomial model. Which of the four models is selected by the cross validation procedure? Which model is selected according to maximum adjusted $R^{2}$?

## Answer
思路：求解过程和例题相同，只需将Log-Log model换成cubic polynomial model，然后计算4种模型的average squared prediction error和adjusted $R^{2}$，以此来选择模型。
```{r}
library(DAAG)
attach(ironslag)
n <- length(magnetic)
e1 <- e2 <- e3 <- e4 <- numeric(n)
ssr1 <- ssr2 <- ssr3 <- ssr4 <- numeric(n)
sst1 <- sst2 <- sst3 <- sst4 <- numeric(n)
ymean <- mean(magnetic)
for (k in 1:n) {
  y <- magnetic[-k]
  x <- chemical[-k]
  # Linear model
  J1 <- lm(y ~ x)
  yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]
  e1[k] <- magnetic[k] - yhat1
  ssr1[k] <- (magnetic[k] - yhat1)^2
  sst1[k] <- (magnetic[k] - ymean)^2
  # Quadratic model
  J2 <- lm(y ~ x + I(x^2))
  yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] + J2$coef[3] * chemical[k]^2
  e2[k] <- magnetic[k] - yhat2
  ssr2[k] <- (magnetic[k] - yhat2)^2
  sst2[k] <- (magnetic[k] - ymean)^2
  # Exponential model
  J3 <- lm(log(y) ~ x)
  logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]
  yhat3 <- exp(logyhat3)
  e3[k] <- magnetic[k] - yhat3
  ssr3[k] <- (magnetic[k] - yhat3)^2
  sst3[k] <- (magnetic[k] - ymean)^2
  # Cubic polynomial model
  J4 <- lm(y ~ x + I(x^2) + I(x^3))
  yhat4 <- J4$coef[1] + J4$coef[2] * chemical[k] + J4$coef[3] * chemical[k]^2 + J4$coef[4] * chemical[k]^3
  e4[k] <- magnetic[k]- yhat4
  ssr4[k] <- (magnetic[k] - yhat4)^2
  sst4[k] <- (magnetic[k] - ymean)^2
}
r1 <- 1-(n-1)/(n-2)*sum(ssr1)/sum(sst1)
r2 <- 1-(n-1)/(n-2)*sum(ssr2)/sum(sst2)
r3 <- 1-(n-1)/(n-2)*sum(ssr3)/sum(sst3)
r4 <- 1-(n-1)/(n-2)*sum(ssr4)/sum(sst4)
round(c(Lin=mean(e1^2), Quad=mean(e2^2), Expo=mean(e3^2), Cubic=mean(e4^2)),3)  ## average squared prediction error
round(c(Lin_R2=r1, Quad_R2=r2, Expo_R2=r3, Cubic_R2=r4),3)  ## adjusted R^2
```
根据the cross validation procedure计算出4种模型的average squared prediction error，我们可以选择Quadratic model，因为它的average squared prediction error最低。  
按照maximum adjusted $R^2$的话，我们可以选择Quadratic model，因为它的adjusted $R^2$最大。

## Question
Implement the two-sample Cramer-von Mises test for equal distributions as a permutation test. Apply the test to the data in Examples 8.1 and 8.2.

## Answer
思路：大致过程与例题相同，只需写一个Cramer-von Mises test函数，然后将检验方法换成自己写的Cramer-von Mises test即可。
```{r}
myFun <- function(x,y){
  n <- length(x)
  m <- length(y)
  f <- ecdf(x)
  g <- ecdf(y)
  w <- n*m/(n+m)^2*(sum((f(x)-g(x))^2) + sum((f(y)-g(y))^2))
}

attach(chickwts)
x <- sort(as.vector(weight[feed == "soybean"]))
y <- sort(as.vector(weight[feed == "linseed"]))
detach(chickwts)
set.seed(12345)
R <- 999;z <- c(x, y);K <- 1:26;n<-length(x)
reps <- numeric(R);t0 <- myFun(x, y)
for (i in 1:R) {
  xy <- sample(z);
  x1 <- xy[1:n]; y1 <- xy[-(1:n)]
  reps[i] <- myFun(x1,y1)
}
p <- mean(c(t0, reps) >= t0)
print(p)
hist(reps, main = "the two-sample Cramer-von Mises test", freq = FALSE, xlab = "reps (p = 0.419)",breaks = "scott")
points(t0, 0, cex = 1, pch = 16)
```
  
分析：p=0.419不支持分布不同的备选假设，即原假设成立，分布相同。

## Question
Implement the bivariate Spearman rank correlation test for independence[255] as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the achieved significance level of the permutation test with the p-value reported by cor.test on the same samples.

## Answer
思路：先自定义两个向量x和y，利用cor函数计算Spearman等级相关性，然后按照permutation test的过程计算对应的p值(p_per.test)，把它与cor.test的p值(p_cor.test)相比较。
```{r}
x <- c(67, 70, 75, 78, 73, 89, 84, 99, 90, 91)
y <- c(22, 27, 30, 23, 25, 31, 38, 35, 34, 32)
cor0 <- cor(x, y, method = "spearman")
set.seed(12345)
R <- 1e3
n <- length(x)
z <- c(x, y)
cor1 <- numeric(R)
for (i in 1:R) {
  xy <- sample(z);
  x1 <- xy[1:n]; y1 <- xy[-(1:n)]
  cor1[i] <- cor(x1, y1, method = "spearman")
}
p <- mean(abs(c(cor0, cor1)) >= abs(cor0))
p_cor.test <- cor.test(x, y, method = "spearman")$p.value
round(c(correlation=cor0,p_per.test=p,p_cor.test=p_cor.test),3)
```
分析：从结果中，我们可以看到Spearman等级相关性为0.782，相应的p值为0.012。这表明两个向量之间存在很强的正相关性，并且相关性的p值小于0.05，因此相关性具有统计显著性。permutation test的p值为0.014，与cor.test的p值非常接近。

## Question 9.3
Use the Metropolis-Hastings sampler to generate random variables from a standard Cauchy distribution. Discard the first 1000 of the chain, and compare the deciles of the generated observations with the deciles of the standard Cauchy distribution (see qcauchy or qt with df=1). Recall that a Cauchy($\theta$,$\eta$) distribution has density function
$$ f(x) = \frac{1}{\theta\pi(1+[(x-\eta)/\theta]^2)},\qquad-\infty<x<\infty,\theta>0$$
The standard Cauchy has the Cauchy($\theta=1$,$\eta=0$) density. (Note that the standard Cauchy density is equal to the Student t density with one degree of freedom.)

## Answer
思路：确定目标分布为标准柯西分布（其密度函数相当于自由度为1的t分布），然后设置提议分布为正态分布，其均值等于当前状态，标准差为一个小常数，这里取1。然后根据M-H算法流程生成马尔可夫链，丢弃前1000个样本，生成Q-Q图比较观测值的分位数与标准柯西分布的理论分位数，生成标准柯西分布密度叠加的样本直方图。
```{r}
f <- function(x){  ## standard Cauchy density function
  return(dt(x, df = 1))
}

set.seed(12345)
m <- 1e4
x <- numeric(m)
x[1] <- rnorm(1,0,1)
k <- 0
u <- runif(m)

for (i in 2:m) {
  xt <- x[i-1]
  y <- rnorm(1,xt,1)  ## proposal distribution N(xt,1)
  num <- f(y) * dnorm(xt, mean = y, sd = 1)
  den <- f(xt) * dnorm(y, mean = xt, sd = 1)
  if(u[i] <= num/den)
    x[i] <- y      ## y被接受
  else{
    x[i] <- xt     ## y被拒绝
    k <- k+1       ## 被拒绝次数
  }
}
cat("reject probablity:",k/m)

b <- 1001          ## discard the burnin sample
y1 <- x[b:m]
a <- ppoints(100)
QR <- qcauchy(a)   ##quantiles of Cauchy
Q <- quantile(x, a)
qqplot(QR, Q, main="", xlab="Cauchy Quantiles", ylab="Sample Quantiles", xlim=c(-20,20), ylim=c(-20,20))
hist(y1, breaks="scott", main="", xlab="", freq=FALSE)
lines(QR, f(QR))
```
  
分析：拒绝概率落入10%−40%的区间内，符合马氏链的要求。通过观察直方图，可以认为抽样分布基本符合标准柯西分布。从Q-Q图中可以看出，样本分位数与标准柯西分布理论分位数大致一致。

## Question 9.8
This example appears in [40]. Consider the bivariate density
$$f(x,y)\propto\dbinom{n}{x}y^{x+a-1}(1-y)^{n-x+b-1},\quad x=0,1,...,n,0\leq y\leq 1.$$
It can be shown (see e.g. [23]) that for fixed a,b,n, the conditional distributions are Binomial$(n,y)$ and Beta$(x+a,n−x+b)$. Use the Gibbs sampler to generate a chain with target joint density $f(x,y)$.

## Answer
思路：先设置a,b,n的值，以及链的初始状态和长度。根据给的条件分布来生成x和y的随机数，然后按照Gibbs采样方法生成具有目标联合密度$f(x,y)$的马氏链。
```{r}
set.seed(12345)
N <- 1e4               #length of chain
burn <- 1000            #burn-in length
X <- matrix(0, N, 2)    #the chain, a bivariate sample
a <- 2
b <- 4
n <- 16

x0 <- sample(0:n,1)
y0 <- runif(1)
X[1,] <- c(x0,y0)
for (i in 2:N) {
  y <- X[i-1,2]
  X[i,1] <- rbinom(1,n,y)
  x <- X[i,1]
  X[i,2] <- rbeta(1,x+a,n-x+b)
}
b <- burn + 1
x1 <- X[b:N, ]
colMeans(x1)
cov(x1)
cor(x1)
plot(x1, main="", cex=.5, xlab=bquote(X[1]),ylab=bquote(X[2]), ylim=range(x1[,2]))
```
  
分析：样本的均值、方差和相关性接近真实参数，图中显示出具有正相关的双变量Binomial-Beta分布的椭圆形对称性。

## Question
For each of the above exercise, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat{R}<1.2$.

## Answer
Gelman-Rubin method for Question 9.3
```{r}
Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi)     #row means
  B <- n * var(psi.means)        #between variance est.
  psi.w <- apply(psi, 1, "var")  #within variances
  W <- mean(psi.w)               #within est.
  v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
  r.hat <- v.hat / W             #G-R statistic
  return(r.hat)
}

chain <- function(sigma, N, X1) {
  #generates a Metropolis chain for standard Cauchy
  #with Normal(X[t], sigma) proposal distribution
  #and starting value X1
  x <- rep(0, N)
  x[1] <- X1
  u <- runif(N)
  for (i in 2:N) {
    xt <- x[i-1]
    y <- rnorm(1, xt, sigma)    #candidate point
    r1 <- dt(y, df = 1) * dnorm(xt, y, sigma)
    r2 <- dt(xt, df = 1) * dnorm(y, xt, sigma)
    r <- r1/r2
    if (u[i] <= r) x[i] <- y else
      x[i] <- xt
  }
  return(x)
}

set.seed(12345)
sigma <- 1        #parameter of proposal distribution
k <- 4            #number of chains to generate
n <- 21000        #length of chains
b <- 12000        #burn-in length
x0 <- rnorm(k)

#generate the chains
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k)
  X[i, ] <- chain(sigma, n, x0[i])

#compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))
cat("R=",Gelman.Rubin(psi))

#plot psi for the four chains
for (i in 1:k)
  plot(psi[i, (b+1):n], type="l",xlab=i, ylab=bquote(psi))

#plot the sequence of R-hat statistics
rhat <- rep(0, n)
for (j in (b+1):n)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```
  
分析：在time 12001 to 21000的$\hat{R}$图中表明，链在大约2100次迭代内已经大致收敛到目标分布。图中的虚线位于$\hat{R}=1.2$。  
Gelman-Rubin method for Question 9.8
```{r}
Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi)     #row means
  B <- n * var(psi.means)        #between variance est.
  psi.w <- apply(psi, 1, "var")  #within variances
  W <- mean(psi.w)               #within est.
  v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
  r.hat <- v.hat / W             #G-R statistic
  return(r.hat)
}

chain <- function(a, b, n, N, X1, Y1) {
  #generates a Metropolis chain for target distribution
  #with the conditional distributions are Binomial(n,y) and Beta(x+a,n−x+b)
  #and starting value X1,Y1
  x <- matrix(0, N, 2)
  x[1,] <- c(X1,Y1)
  for (i in 2:N) {
    Y <- x[i-1,2]
    x[i,1] <- rbinom(1,n,Y)
    X <- x[i,1]
    x[i,2] <- rbeta(1,X+a,n-X+b)
  }
  return(x)
}

set.seed(12345)
a <- 2
b <- 4
n <- 16
k <- 4            #number of chains to generate
N <- 10000        #length of chains
burn <- 2000      #burn-in length
x0 <- sample(0:n,k,replace = TRUE)
y0 <- runif(k)

#generate the chains
X <- array(0,dim = c(N,2,k))
for (i in 1:k)
  X[ , ,i] <- chain(a, b, n, N, x0[i], y0[i])

#compute diagnostic statistics
psi <- t(apply(X, 3, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman.Rubin(psi))

#plot psi for the four chains
for (i in 1:k)
  plot(psi[i, (burn+1):(2*N)], type="l",xlab=i, ylab=bquote(psi))

#plot the sequence of R-hat statistics
rhat <- rep(0, (2*N))
for (j in (burn+1):(2*N))
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(burn+1):(2*N)], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```
  
分析：在time 2000 to 20000的$\hat{R}$图中表明，链在大约9000次迭代内已经大致收敛到目标分布。图中的虚线位于$\hat{R}=1.2$。

## Question
* Algorithm (continuous situation)
    + Target pdf: $f(x)$.
    + Replace $i$ and $j$ with $s$ and $r$.
    + Proposal distribution (pdf): $g(r|s)$.
    + Acceptance probability: $\alpha(s,r)=\min\{\frac{f(r)g(s|r)}{f(s)g(r|s)},1\}$.
    + Transition kernel (mixture distribution): $K(r,s)=I(s\not=r)\alpha(r,s)g(s|r)+I(s=r)[1-\int \alpha(r,s)g(s|r)]$.
    + Stationarity: $K(s,r)f(s)=K(r,s)f(r)$.
Proof the Stationarity of Metropolis-Hastings sampler Algorithm in continuous situation.

## Answer
由Transition kernel (mixture distribution)可得等式：
$$K(s,r)f(s)=I(r\not=s)\alpha(s,r)g(r|s)f(s)+I(r=s)[1-\int \alpha(s,r)g(r|s)f(s)]$$
又根据Acceptance probability: $\alpha(s,r)=\min\{\frac{f(r)g(s|r)}{f(s)g(r|s)},1\}$可得
\begin{align}
K(s,r)f(s) &= I(r\not=s)\alpha(s,r)g(r|s)f(s)+I(r=s)[1-\int \alpha(s,r)g(r|s)f(s)]\\
&= I(r\not=s)\min\{\frac{f(r)g(s|r)}{f(s)g(r|s)},1\}g(r|s)f(s)+I(r=s)[1-\int\min\{\frac{f(r)g(s|r)}{f(s)g(r|s)},1\}g(r|s)f(s)]\\
&= I(r\not=s)\min\{f(r)g(s|r),g(r|s)f(s)\}+I(r=s)[1-\int\min\{f(r)g(s|r),g(r|s)f(s)\}]\\
&= I(r\not=s)\min\{1,\frac{f(s)g(r|s)}{f(r)g(s|r)}\}g(s|r)f(r)+I(r=s)[1-\int\min\{1,\frac{f(s)g(r|s)}{f(r)g(s|r)}\}g(s|r)f(r)]\\
&= I(s\not=r)\alpha(r,s)g(s|r)f(r)+I(s=r)[1-\int \alpha(r,s)g(s|r)f(r)]\\
&= K(r,s)f(r)
\end{align}
至此，成功证明了在continuous situation中，Metropolis-Hastings采样算法的平稳性，即$K(s,r)f(s)=K(r,s)f(r)$.

## Exercise 11.3
## Question
(a) Write a function to compute the $k^{th}$ term in
$$\sum_{k=0}^\infty \frac{(-1)^k}{k!2^k}\frac{||a||^{2k+2}}{(2k+1)(2k+2)}\frac{\Gamma(\frac{d+1}{2})\Gamma(k+\frac{3}{2})}{\Gamma(k+\frac{d}{2}+1)},$$
where d ≥ 1 is an integer, $a$ is a vector in $R^d$, and $||·||$ denotes the Euclidean norm. Perform the arithmetic so that the coefficients can be computed for (almost) arbitrarily large k and d. (This sum converges for all $a \in R^d$).
(b) Modify the function so that it computes and returns the sum.
(c) Evaluate the sum when $a =(1,2)^T$.

## Answer
思路：先编写函数计算第k个项，然后再写一个函数用来计算整个级数的和，由于上界是无穷大，所以需要设置一个阈值来停止累加，我设置的阈值为1e-10，即当项绝对值小于1e-10时，认为其对求和的贡献可忽略不计。最后设置d的值，计算当向量a=(1,2)时的总和。需要注意的是，要满足整数d ≥ 1且可以很大，那么就需要对我们第一个函数进行修改，将gamma函数改成lgamma函数，就可以处理非常大的数值，因为它避免了直接计算阶乘，而是计算其对数值。
```{r}
## function to compute the kth term
compute_kth_term <- function(k, d, a) {
  norm_a <- sqrt(sum(a^2))  # 计算向量a的欧几里得范数
  kth_term <- (-1)^k / exp(lgamma(k+1)) / (2^k) * (norm_a^(2*k+2)) / ((2*k+1)*(2*k+2)) * exp(lgamma((d+1)/2) + lgamma(k + 3/2) - lgamma(k + d/2 + 1))
  return(kth_term)
}

## function to compute and return the sum
compute_sum <- function(d, a, tol = 1e-10) {
  norm_a <- sqrt(sum(a^2))
  sum <- 0
  k <- 0
  while (TRUE) {
    term <- compute_kth_term(k, d, a)
    if (abs(term) < tol) {
      break    # 设置一个阈值来确定何时停止累加，当项绝对值小于1e-10时，认为其对求和的贡献可忽略不计。
    }
    sum <- sum + term
    k <- k + 1
  }
  return(sum)
}

## Evaluate the sum when a = (1,2)
a <- c(1,2)
d <- 1     # d ≥ 1 is an integer
print(compute_sum(d,a))

d <- 1e4
print(compute_sum(d,a))
```
分析：当a=(1,2)，d=1时，总和为1.813545；当d=1e4时，总和为0.03132816。

## Exercise 11.5
## Question
Write a function to solve the equation
\begin{aligned}
\frac{2 \Gamma\left(\frac{k}{2}\right)}{\sqrt{\pi(k-1)} \Gamma\left(\frac{k-1}{2}\right)} \int_{0}^{c_{k-1}}\left(1+\frac{u^{2}}{k-1}\right)^{-k / 2} d u \\
=\frac{2 \Gamma\left(\frac{k+1}{2}\right)}{\sqrt{\pi k} \Gamma\left(\frac{k}{2}\right)} \int_{0}^{c_{k}}\left(1+\frac{u^{2}}{k}\right)^{-(k+1) / 2} d u
\end{aligned}
for $a$, where $$ c_{k}=\sqrt{\frac{a^{2} k}{k+1-a^{2}}} $$ 
Compare the solutions with the points $A(k)$ in Exercise 11.4.

## Answer
思路：先利用uniroot函数求例题11.4的根，然后再求11.5的根，需要注意的是当k很大的时候，计算$\Gamma(k)$会出现报错，因为算术溢出，所以需要log取对数转换一下。然后利用uniroot函数求根时，需要寻找到合适的上下界，确保两端点的正负号相反。
```{r}
k <- c(4:25, 100, 500, 1000)
n <- length(k)
A <- rep(0, n)
eps <- .Machine$double.eps ^ 0.25

for (i in 1:n) {
  fa <- function(a) {
    num1 <- sqrt(a ^ 2 * (k[i] - 1) / (k[i] - a ^ 2))
    num2 <- sqrt(a ^ 2 * k[i] / (k[i] + 1 - a ^ 2))
    pt(num2, k[i]) - pt(num1, k[i] - 1)
  }
  out <- uniroot(fa,lower = eps, upper = sqrt(k[i]-eps))
  A[i] <- out$root        # 11.4的根
}

B <- rep(0, n)            # 11.5的根
for (i in 1:n) {
  f <- function(x){
    return(exp(log(2) + lgamma(x/2) - (1/2)*(log(pi*(x-1))) - lgamma((x-1)/2)))
  }
  h <- function(a){
    num1 <- (f(k[i])*integrate(function(u)(1 + (u^2)/(k[i]-1))^(-k[i]/2), lower = 0, upper = sqrt((a^2)*(k[i]-1)/(k[i]-a^2)))$value)
    num2 <- (f(k[i]+1)*integrate(function(u)(1 + (u^2)/k[i])^(-(k[i]+1)/2), lower = 0, upper = sqrt((a^2)*k[i]/(k[i]+1-a^2)))$value)
    return(num1 - num2)
  }
  if(i<20)  # 为不同的k值寻找不同的上下界
    B[i] <- uniroot(h, lower = eps, upper = sqrt(k[i]-eps)-0.1)$root
  if(20<=i & i<=21)
    B[i] <- uniroot(h, lower = eps, upper = sqrt(k[i]-eps))$root
  if(22<=i & i<=23)
    B[i] <- uniroot(h, lower = 4, upper = sqrt(k[i]))$root
  if(i==24)
    B[i] <- uniroot(h, lower = sqrt(k[i])-0.03, upper = sqrt(k[i])-0.001)$root
  if(i==25)
    B[i] <- uniroot(h, lower = sqrt(k[i])-0.0037, upper = sqrt(k[i]))$root
}

knitr::kable(data.frame(k,A,B))
```
分析：根据表格中的数据，可以发现当k值相同时，两者的根相同。

## Question
Suppose $T_1,...,T_n$ are i.i.d. samples drawn from the exponential distribution with expectation λ. Those values greater than $\tau$ are not observed due to right censorship, so that the observed values are $Y_i = T_iI(T_i ≤ \tau) + \tau I(T_i > \tau), i = 1,...,n.$ Suppose $\tau = 1$ and the observed $Y_i$ values are as follows:
$$0.54, 0.48,0.33,0.43,1.00,1.00,0.91,1.00,0.21,0.85$$
Use the E-M algorithm to estimate λ, compare your result with the observed data MLE (note: $Y_i$ follows a mixture distribution).

## Answer
思路：首先初始化参数λ，然后通过EM算法迭代更新λ的估计值，直到收敛。在E-step中，计算每个样本是否被删失的概率，这通过指数分布的累积分布函数来实现。在M-step中，根据这些概率更新λ的估计值。最后，将EM算法的结果与MLE的结果进行比较。
```{r}
# 观察到的数据
Y <- c(0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85)

# 初始化参数
n <- length(Y) # 样本数量
tau <- 1
lambda <- 1 / mean(Y[Y <= tau]) # 初始估计λ_0

# EM算法
for (i in 1:1000) {
  # E-step: 计算每个样本的潜在分类概率（是否被删失）
  Z <- pexp(Y, rate = lambda)
  
  # M-step: 更新lambda参数
  lambda_new <- n / sum(Y * (1 - Z) + tau * Z)
  
  # 检查收敛性
  if (abs(lambda_new - lambda) < 1e-6) {
    break
  }
  
  # 更新参数和对数似然
  lambda <- lambda_new
}

# 输出结果
cat("Estimated lambda using EM algorithm:", lambda, "\n")

# 计算MLE
lambda_mle <- 1 / mean(Y[Y <= tau])
cat("Estimated lambda using MLE:", lambda_mle, "\n")
```
分析：EM算法估计的参数值与观察数据的最大似然估计(MLE)相比，有一点差距，分析原因可能是更新λ参数的公式需要改进。

## Question
Use the simplex algorithm to solve the following problem.  
Minimize $4x+2y+9z$ subject to
$$2x+y+z≤2$$
$$x−y+3z≤3$$
$$x≥0,y≥0,z≥0.$$

## Answer
思路：将约束条件和目标函数的参数写成向量形式，代入simplex()函数，因为求的是最小化，所以maxi选项选择FALSE。
```{r}
library(boot)
A1 <- rbind(c(2, 1, 1), c(1, -1, 3))
b1 <- c(2, 3)
a <- c(4, 2, 9)
simplex(a = a, A1 = A1, b1 = b1, maxi = FALSE)
```
分析：由运行结果可知，当x=y=z=0时，目标函数值最小，最小值为0.

## Question
Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:
```{r,eval=FALSE}
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)
```

## Answer
思路：将formulas列表中的每一个元素代入到lm()函数中，一种方法是利用for循环，另外一种方法是直接使用lapply函数，比较两者生成的结果是否相同。
```{r}
library(datasets)
data(mtcars)

formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)

## use for loops to fit linear models to the mtcars
out1 <- vector("list", length(formulas))
for (i in seq_along(formulas)) {
  out1[[i]] <- lm(formulas[[i]], mtcars)
}

## use lapply() to fit linear models to the mtcars
out2 <- lapply(formulas, function(i) {lm(i, mtcars)})

out1
out2
```
分析：使用for循环和使用lapply函数的最终结果是一致的，但是前者在迭代列表中的元素时相对冗长繁琐，而lapply函数更加简洁，避免了一段循环代码。

## Question
Fit the model mpg ~ disp to each of the bootstrap replicates of mtcars in the list below by using a for loop and lapply(). Can you do it without an anonymous function?
```{r,eval=FALSE}
bootstraps <- lapply(1:10, function(i) {
  rows <- sample(1:nrow(mtcars), rep = TRUE)
  mtcars[rows, ]
})
```

## Answer
思路：与上一题类似，不同之处在于使用lapply函数时不使用匿名函数，这样的话直接调用lm函数，只需在后面加上参数formula = mpg ~ disp即可。
```{r}
library(datasets)
data(mtcars)

set.seed(12345)
bootstraps <- lapply(1:10, function(i) {
  rows <- sample(1:nrow(mtcars), rep = TRUE)
  mtcars[rows, ]
})

## use for loops
out1 <- vector("list", length(bootstraps))
for (i in seq_along(bootstraps)) {
  out1[[i]] <- lm(mpg ~ disp, bootstraps[[i]])
}

## use lapply() without an anonymous function
out2 <- lapply(bootstraps, lm, formula = mpg ~ disp)

out1
out2
```
分析：out1和out2依然相同，使用lapply函数更加简洁明了。

## Question
For each model in the previous two exercises, extract R2 using the function below.
```{r,eval=FALSE}
rsq <- function(mod) summary(mod)$r.squared
```

## Answer
思路：先计算模型，然后调用提供的rsq函数提取每个模型的$R^2$即可。

### First model
```{r}
library(datasets)
data(mtcars)

formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)

rsq <- function(mod) summary(mod)$r.squared

## use lapply() to fit linear models to the mtcars
out <- lapply(formulas, function(i) {rsq(lm(i, mtcars))})

out
```
### Second model
```{r}
library(datasets)
data(mtcars)

set.seed(12345)
bootstraps <- lapply(1:10, function(i) {
  rows <- sample(1:nrow(mtcars), rep = TRUE)
  mtcars[rows, ]
})

rsq <- function(mod) summary(mod)$r.squared

## use lapply() without an anonymous function
out <- lapply(bootstraps, lm, formula = mpg ~ disp)

## extract R2
res <- numeric(length(out))
for (i in 1:length(out)) {
  res[i] <- rsq(out[[i]])
}

res
```
分析：两种模型的$R^2$提取结果如上面代码所示。

## Question
The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.
```{r,eval=FALSE}
trials <- replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)
```
Extra challenge: get rid of the anonymous function by using [[ directly.

## Answer
思路：调用sapply函数提取每一个trial的p值，省去了for循环，并且简化输出结果。

### Use sapply() and an anonymous function
```{r}
set.seed(12345)
trials <- replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)
out <- sapply(trials, function(i) {i$p.value})
out
```
### Use sapply() without an anonymous function
```{r}
set.seed(12345)
trials <- replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)
out <- sapply(trials, '[[',3)
out
```
分析：输出结果为一个包含所有p值的向量，而不是一个列表，简化了输出结果。

## Question
Implement a combination of Map() and vapply() to create an lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?

## Answer
思路：首先需要定义一个函数，它需要接受的参数有：要应用的函数f，一系列列表（函数中需要对列表长度进行比较，长度相同才能正常调用，否则输出提示信息），传递给vapply()的其他参数，例如USE.NAMES或FUN.VALUE。
```{r}
my_lapply <- function(f, df, fun_value) {
  # 检查输入列表长度是否一致
  list_lengths <- sapply(list(df), length)
  if (length(unique(list_lengths)) > 1) {
    stop("All input lists must have the same length.")
  }
  
  # 使用 Map() 并行应用函数f到所有输入
  mapped_results <- Map(f, df)
  
  # 使用 vapply() 将结果转换为向量或矩阵
  final_results <- vapply(mapped_results, function(x) x, FUN.VALUE=fun_value)
  return(final_results)
}

set.seed(12345)
df <- data.frame(replicate(6,sample(c(1:10),10,rep=T)))
fun_value <- c(min=0,Qu1=0,median=0,mean=0,Qu3=0,max=0)
my_lapply(summary, df, fun_value)
```
分析：上面的my_lapply结合了Map()和vapply()，实现了可以多个列表输入并将输出结果保存在一个矩阵中的功能。

## Question
Make a faster version of chisq.test() that only computes the chi-square test statistic when the input is two numeric vectors with no missing values. You can try simplifying chisq.test() or by coding from the mathematical definition (http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test).

## Answer
思路：依据卡方检验的数学定义公式来编写我的卡方检验函数，由于输入数据为两个相同长度的数字型向量，所以不要考虑其他类型情况，输出为一个包含卡方值、自由度、p值的列表。p值可以借助pchisq函数算出。卡方值计算公式如下：
$$x^2=\sum\frac{(A-T)^2}{T}$$
其中，A为实际值，T为理论值。
```{r}
my_chisq_test <- function(x, y) {
  m <- rbind(x, y)
  # Calculate row totals
  row_total <- matrix(rowSums(m))
  
  # Calculate column totals
  col_total <- t(matrix(colSums(m)))
  
  # Calculate grand total
  grand_total <- sum(m)
  
  # Calculate expected frequencies
  m_expected <- (row_total %*% col_total) / grand_total
  
  # Calculate chi-square statistic
  chi_square <- sum((m - m_expected)^2 / m_expected)
  df <- (length(row_total) - 1) * (length(col_total) - 1)
  p <- pchisq(chi_square, df = df, lower.tail = FALSE)
  return(list(chi_square = chi_square, df = df, p_value = p))
}


x <- c(1,2,3,4,5)
y <- c(6,7,8,9,10)
my_chisq_test(x,y)
```
分析：这个卡方检验函数只用于输入为两个相同长度的数字型向量，没有考虑其他情况，且只输出3个卡方检验统计量，所以函数体内代码较少，没有chisq.test()复杂，相对运行速度更快。

## Question
Can you make a faster version of table() for the case of an input of two integer vectors with no missing values? Can you use it to speed up your chi-square test?

## Answer
思路：先将两个向量各自的重复项去掉，用来确定table的行列数，然后用matrix函数初始化table，再利用一个循环将向量里的元素一个个对应位置填入table中。
```{r}
my_table <- function(x, y) {
  # Find the unique values in both vectors to determine the range for the table
  unique1 <- unique(x)
  unique2 <- unique(y)
  
  # Initialize the table
  # The table will have length(unique1) rows and length(unique2) columns
  table <- matrix(0, nrow = length(unique1), ncol = length(unique2),dimnames = list(unique1, unique2))
  
  # Fill in the table with counts
  for (i in seq_along(x)) {
    row_index <- match(x[i], unique1)
    col_index <- match(y[i], unique2)
    table[row_index, col_index] <- table[row_index, col_index] + 1
  }
  
  return(table)
}

x <- c(1,2,3,4,4)
y <- c('A','B','C','D','D')
my_table(x,y)
```
分析：此方法生成的table与原函数table()生成的一致，且可以用来加速上一题编写的卡方检验函数，只需将原本计算row totals、column totals、grand total的代码替换成下面的代码，其余不变即可：
```{r,eval=FALSE}
table <- my_table(x, y)
  
# Calculate row and column totals
row_totals <- apply(table, 1, sum)
col_totals <- apply(table, 2, sum)
grand_total <- sum(table)
```

## Question 9.8
This example appears in [40]. Consider the bivariate density
$$f(x,y)\propto\dbinom{n}{x}y^{x+a-1}(1-y)^{n-x+b-1},\quad x=0,1,...,n,0\leq y\leq 1.$$
It can be shown (see e.g. [23]) that for fixed a,b,n, the conditional distributions are Binomial$(n,y)$ and Beta$(x+a,n−x+b)$. Use the Gibbs sampler to generate a chain with target joint density $f(x,y)$.  
* Write an Rcpp function for Exercise 9.8.  
* Compare the corresponding generated random numbers with those by the R function you wrote using the function “qqplot”.  
* Campare the computation time of the two functions with the function “microbenchmark”.  
* Comments your results.

## Answer
```{r,eval=FALSE}
## R function for Exercise 9.8
Gibbs_chain_R <- function(a, b, n, N, burn, x0, y0){
  X <- matrix(0, N, 2)   #the chain, a bivariate sample
  X[1,] <- c(x0,y0)
  for (i in 2:N) {
    y <- X[i-1,2]
    X[i,1] <- rbinom(1,n,y)
    x <- X[i,1]
    X[i,2] <- rbeta(1,x+a,n-x+b)
  }
  b <- burn + 1
  x1 <- X[b:N, ]
  return(x1)
}
```
Rcpp function for Exercise 9.8
```{r,eval=FALSE}
#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericMatrix Gibbs_chain_C(double a, double b, int n, int N, int burn, int x0, double y0) {
  NumericMatrix X(N, 2);
  X(0,0) = x0;
  X(0,1) = y0;
  int x;
  double y;
  for(int i = 1; i < N; i++){
    y = X(i-1,1);
    X(i,0) = rbinom(1,n,y)[0];
    x = X(i,0);
    X(i,1) = rbeta(1,x+a,n-x+b)[0];
  }
  NumericMatrix x1(N-burn, 2);
  for(int i = burn; i < N; i++){
    x1(i - burn,0) = X(i,0);
    x1(i - burn,1) = X(i,1);
  }
  return x1;
}
```
Compare the corresponding generated random numbers with those by the R function you wrote using the function “qqplot”.  
Campare the computation time of the two functions with the function “microbenchmark”.
```{r}
Gibbs_chain_R <- function(a, b, n, N, burn, x0, y0){
  X <- matrix(0, N, 2)   #the chain, a bivariate sample
  X[1,] <- c(x0,y0)
  for (i in 2:N) {
    y <- X[i-1,2]
    X[i,1] <- rbinom(1,n,y)
    x <- X[i,1]
    X[i,2] <- rbeta(1,x+a,n-x+b)
  }
  b <- burn + 1
  x1 <- X[b:N, ]
  return(x1)
}

library(SA24168406)
library(Rcpp)
library(microbenchmark)
set.seed(12345)
N <- 1e4                #length of chain
burn <- 1000            #burn-in length
a <- 2
b <- 4
n <- 16
x0 <- sample(0:n,1)
y0 <- runif(1)

xR <- Gibbs_chain_R(a, b, n, N, burn, x0, y0)
xC <- Gibbs_chain_C(a, b, n, N, burn, x0, y0)
qqplot(xR, xC, main = "QQ Plot Comparison", xlab="R Quantiles", ylab="C Quantiles")

ts <- microbenchmark(gibbR=Gibbs_chain_R(a, b, n, N, burn, x0, y0),gibbC=Gibbs_chain_C(a, b, n, N, burn, x0, y0))
summary(ts)[,c(1,3,5,6)]
```
分析：通过Q-Q图可以直观地发现利用R函数和利用C++函数生成的随机数都来自同一分布，因为图上的点大致落在直线y = x上，这表明这两个分布是相似的。  
通过计时函数可以明显发现使用c++语言编写的函数运行速度比R语言编写的函数快很多，差不多是R函数的1/8时间。因此，我认为c++语言的效率在大多数情况下要比R语言高，尤其是在有很多for循环的情况下，提升更加明显。
